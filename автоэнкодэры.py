# -*- coding: utf-8 -*-
"""автоэнкодэры

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MYmPQ0dvEpVd48N_LIW7qVl2UE11czAV

# Автоэнкодеры

# Часть 1. Vanilla Autoencoder

## 1.1. Подготовка данных
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from torch.autograd import Variable
from torchvision import datasets
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data_utils
import torch
import matplotlib.pyplot as plt

import os
import pandas as pd
import skimage.io
from skimage.transform import resize


# %matplotlib inline

def fetch_dataset(attrs_name = "lfw_attributes.txt",
                      images_name = "lfw-deepfunneled",
                      dx=80,dy=80,
                      dimx=64,dimy=64
    ):

    #download if not exists
    if not os.path.exists(images_name):
        print("images not found, donwloading...")
        os.system("wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz")
        print("extracting...")
        os.system("tar xvzf tmp.tgz && rm tmp.tgz")
        print("done")
        assert os.path.exists(images_name)

    if not os.path.exists(attrs_name):
        print("attributes not found, downloading...")
        os.system("wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s" % attrs_name)
        print("done")

    #read attrs
    df_attrs = pd.read_csv("lfw_attributes.txt",sep='\t',skiprows=1,)
    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])


    #read photos
    photo_ids = []
    for dirpath, dirnames, filenames in os.walk(images_name):
        for fname in filenames:
            if fname.endswith(".jpg"):
                fpath = os.path.join(dirpath,fname)
                photo_id = fname[:-4].replace('_',' ').split()
                person_id = ' '.join(photo_id[:-1])
                photo_number = int(photo_id[-1])
                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})

    photo_ids = pd.DataFrame(photo_ids)
    # print(photo_ids)
    #mass-merge
    #(photos now have same order as attributes)
    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))

    assert len(df)==len(df_attrs),"lost some data when merging dataframes"

    # print(df.shape)
    #image preprocessing
    all_photos =df['photo_path'].apply(skimage.io.imread)\
                                .apply(lambda img:img[dy:-dy,dx:-dx])\
                                .apply(lambda img: resize(img,[dimx,dimy]))

    all_photos = np.stack(all_photos.values)#.astype('uint8')
    all_attrs = df.drop(["photo_path","person","imagenum"],axis=1)

    return all_photos, all_attrs

# The following line fetches you two datasets: images, usable for autoencoder training and attributes.
# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind

data, attrs = fetch_dataset()

"""
Разобьем выборку картинок на train и val, выведем несколько картинок в output, чтобы посмотреть, как они выглядят, и получим DataLoader'ы:"""

from sklearn.model_selection import train_test_split
train_photos, val_photos, train_attrs, val_attrs = train_test_split(data, attrs,
                                                                    train_size=0.9, shuffle=False)
train_loader = torch.utils.data.DataLoader(train_photos, batch_size=32, drop_last=True)
val_loader = torch.utils.data.DataLoader(val_photos, batch_size=32, drop_last=True)

import matplotlib.pyplot as plt
from IPython.display import clear_output

plt.figure(figsize=(18, 6))
for i in range(6):
    plt.subplot(2, 6, i+1)
    plt.axis("off")
    plt.imshow(data[i])

plt.show();

"""## 1.2. Архитектура модели
В этом разделе мы напишем и обучем обычный автоэнкодер.



<img src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2" alt="Autoencoder">



"""

dim_code =  16 #размер нашего латентного пространства

"""Реализуем autoencoder.

В данной работе выполнены простейшие сети с импользованием линейных слоев и Relu.
"""

# define a simple linear VAE
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()

        self.flatten = nn.Flatten()

        # encoder
        self.encoder = nn.Sequential (
            nn.Linear(in_features=12288, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=dim_code)
        )

        # decoder
        self.decoder = nn.Sequential(
            nn.Linear(in_features=dim_code, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=12288)
        )

    def forward(self, sample):
        x = self.flatten(sample).float()
        latent_code = self.encoder(x)
        latent_code =latent_code.view(-1, 2, dim_code)
        reconstruction = self.decoder(latent_code)

        return reconstruction, latent_code

    def sample(self, z):
        generated = self.decoder(z)
        generated = generated.view(-1, 64, 64, 3)
        return generated

    def get_latent_vector(self, sample):
        x = self.flatten(sample).float()
        latent_code = self.encoder(x)
        latent_code =latent_code.view(-1, 2, dim_code)
        return latent_code

"""## 1.3 Обучение

Обучим нашу модель с использованием MSE loss и оптимизатором Adam. Затем выведем для сравнения исходные и восстановленные картинки, а также график тренировочного и валидационного лоссов.
"""

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

criterion = F.mse_loss

autoencoder = Autoencoder().to(device)

optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)

from tqdm.notebook import tqdm
n_epochs = 45
train_losses = []
val_losses = []

for epoch in tqdm(range(n_epochs)):
    autoencoder.train()
    train_losses_per_epoch = []
    for batch in train_loader:
        optimizer.zero_grad()
        reconstruction, latent_code = autoencoder(batch.to(device))
        reconstruction = reconstruction.view( -1, 64, 64, 3)
        loss = criterion(batch.to(device).float(), reconstruction)
        loss.backward()
        optimizer.step()
        train_losses_per_epoch.append(loss.item())

    train_losses.append(np.mean(train_losses_per_epoch))

    autoencoder.eval()
    val_losses_per_epoch = []
    with torch.no_grad():
        for batch in val_loader:
          reconstruction, latent_code = autoencoder(batch.to(device))
          reconstruction = reconstruction.view(-1, 64, 64, 3)
          loss = criterion(batch.to(device).float(), reconstruction)
          val_losses_per_epoch.append(loss.item())

    val_losses.append(np.mean(val_losses_per_epoch))

autoencoder.eval()
  with torch.no_grad():
      for batch in val_loader:
        reconstruction, latent_code = autoencoder(batch.to(device))
        reconstruction = reconstruction.view(-1, 64, 64, 3)
        result = reconstruction.cpu().detach().numpy()
        ground_truth = batch.numpy()
        break

"""Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 20))
for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):
  plt.subplot(5, 2, 2*i+1)
  plt.imshow(gt)
  plt.subplot(5, 2, 2*i+2)
  plt.imshow(res)

plt.figure(figsize=(10, 7))
plt.plot(np.arange(len(train_losses)), train_losses, label='Train')
plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')

plt.xlabel('Epoch')
plt.title('MSE loss')
plt.legend()
plt.show()

"""Ошибка очевидно уменьшается, но, конечно, при использовании более хитрой сети можно было бы добиться лучшего результата.

## 1.4. Sampling

Теперь возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:
"""

# сгенерируем 25 рандомных векторов размера latent_space
z = []
for i in range(25):
  z.append(np.random.randn(16, 2, 16))
  i+1

for i in z:
  output = autoencoder.sample(torch.FloatTensor(i).to(device))

plt.figure(figsize=(18, 6))
for i in range(6):
  plt.subplot(1, 6, i+1)
  plt.axis('off')
  plt.imshow(output[i].cpu().detach().numpy())

plt.show();

"""Результаты страшноватые.... но схожесть с лицами есть)))))

## 1.5 Time to make fun! (3 балла)

Давайте научимся пририсовывать людям улыбки =)

План такой:

1. Нужно выделить "вектор улыбки": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.

2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких

3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей

4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!
"""

#возьмем по 32 картинки людей
attrs['Smiling'] = attrs['Smiling'].astype (float)
largest_smile = attrs.nlargest(32, ['Smiling'])


attrs['Frowning'] = attrs['Frowning'].astype (float)
largest_sad = attrs.nlargest(32, ['Frowning'])

smiling_people_idxs = largest_smile.index
smile_data = data[smiling_people_idxs ]

sad_people_idxs = largest_sad.index
sad_data = data[sad_people_idxs ]

#получаем улыбку
smile_people = autoencoder.get_latent_vector(torch.FloatTensor(smile_data).to(device))

sad_people = autoencoder.get_latent_vector(torch.FloatTensor(sad_data).to(device))

smile = torch.mean(smile_people, dim=0) - torch.mean(sad_people, dim=0)
print(smile)

output_happy = autoencoder.sample(sad_people+smile)
output_sad = autoencoder.sample(sad_people)

plt.figure(figsize=(18, 6))
for i in range(6):
  plt.subplot(1, 6, i+1)
  plt.axis('off')
  plt.imshow(output_happy[i].cpu().detach().numpy())

plt.show();

plt.figure(figsize=(18, 6))
for i in range(6):
  plt.subplot(1, 6, i+1)
  plt.axis('off')
  plt.imshow(output_sad[i].cpu().detach().numpy())

plt.show();

"""Сверху улыбающиеся люди, а снизу оригиналы. Перевес идет в сторону женственности лица, скорее всего из-за дисбаланса классов.

# Часть 2: Variational Autoencoder

Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9
"""

from torchvision.transforms import transforms

batch_size = 32

# convert data to torch.FloatTensor
transform = transforms.ToTensor()

# load the training and test datasets
train_data = datasets.MNIST(root='data', train=True,
                                   download=True, transform=transform)
test_data = datasets.MNIST(root='data', train=False,
                                  download=True, transform=transform)

num_workers = 0

# prepare data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, drop_last=True)

"""## 2.1 Архитектура модели и обучение

Реализуем VAE.
"""

dim_code = 16

class VAE(nn.Module):
    def __init__(self):

        super(VAE, self).__init__()

        self.flatten = nn.Flatten()

        # encoder
        self.encoder = nn.Sequential(
            nn.Linear(in_features=784, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=dim_code*2),

        )

        # decoder
        self.decoder = nn.Sequential(
            nn.Linear(in_features=dim_code, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=784),

        )

    def encode(self, x):
        x = self.encoder(x).view(-1, 2, dim_code)
        mu = x[:,0]
        logsigma = x[:,1]

        return mu, logsigma

    def gaussian_sampler(self, mu, logsigma):
        if self.training:
          std = torch.exp(0.5 * logsigma)
          eps = torch.randn_like(std)
          sample = mu + (eps * std)
          return sample
        else:
          return mu

    def decode(self, z):
        reconstruction = self.decoder(z)
        return reconstruction

    def forward(self, x):
        mu, logsigma = self.encode(x)
        latent_code = self.gaussian_sampler(mu, logsigma)
        reconstruction = self.decode(latent_code)

        reconstruction = torch.sigmoid(reconstruction)

        return mu, logsigma, reconstruction

"""Определим лосс и его компоненты для VAE:

Общий лосс будет выглядеть так:

$$\mathcal{L} = -D_{KL}(q_{\phi}(z|x)||p(z)) + \log p_{\theta}(x|z)$$

Формула для KL-дивергенции:

$$D_{KL} = -\frac{1}{2}\sum_{i=1}^{dimZ}(1+log(\sigma_i^2)-\mu_i^2-\sigma_i^2)$$

В качестве log-likelihood возьмем привычную нам кросс-энтропию.
"""

def KL_divergence(mu, logsigma):
    """
    часть функции потерь, которая отвечает за "близость" латентных представлений разных людей
    """
    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())
    return loss

def log_likelihood(x, reconstruction):
    """
    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)
    """
    loss = nn.BCELoss(reduction='sum')
    return loss(reconstruction, x)

def loss_vae(x, mu, logsigma, reconstruction):
    return KL_divergence(mu, logsigma)+log_likelihood(x, reconstruction)

"""И обучим модель:"""

criterion = loss_vae

autoencoder = VAE().to(device)

optimizer = torch.optim.AdamW(autoencoder.parameters(), lr=3e-4)

from tqdm.notebook import tqdm
n_epochs = 20
train_losses = []
val_losses = []

for epoch in tqdm(range(n_epochs)):
    autoencoder.train()
    train_losses_per_epoch = []

    for batch, _  in train_loader:

        batch = batch.view(batch.size(0), -1)
        optimizer.zero_grad()
        mu, logsigma, reconstruction = autoencoder(batch.to(device))
        loss = criterion(batch.to(device),mu, logsigma, reconstruction)
        loss.backward()
        optimizer.step()
        train_losses_per_epoch.append(loss.item())

    train_losses.append(np.mean(train_losses_per_epoch))

    autoencoder.eval()
    val_losses_per_epoch = []
    with torch.no_grad():
        for batch, _ in test_loader:
          batch = batch.view(batch.size(0), -1)
          mu, logsigma, reconstruction = autoencoder(batch.to(device))
          loss = criterion(batch.to(device),mu, logsigma, reconstruction)
          val_losses_per_epoch.append(loss.item())

    val_losses.append(np.mean(val_losses_per_epoch))

"""Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"""

#< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>

autoencoder.eval()
with torch.no_grad():
    for batch, _ in test_loader:
      print(batch.shape)
      ground_truth = batch.squeeze().numpy()
      batch = batch.view(batch.size(0), -1)
      mu, logsigma, reconstruction = autoencoder(batch.to(device))
      reconstruction = reconstruction.view(-1, 28, 28)
      reconstruction = reconstruction.cpu().detach().numpy()

      break

import matplotlib.pyplot as plt


plt.figure(figsize=(8, 20))
for i, (gt, res) in enumerate(zip(ground_truth[:5], reconstruction[:5])):
  plt.subplot(5, 2, 2*i+1)
  plt.imshow(gt)
  plt.subplot(5, 2, 2*i+2)
  plt.imshow(res)

plt.figure(figsize=(10, 7))
plt.plot(np.arange(len(train_losses)), train_losses, label='Train')
plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')

plt.xlabel('Epoch')
plt.title('MSE loss')
plt.legend()
plt.show()

"""Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"""

z = []
for i in range(25):
  z.append(np.random.randn(32, 16))
  i+1

for i in z:
  output = autoencoder.decode(torch.FloatTensor(i).to(device))
output = output.view(-1, 28, 28)

plt.figure(figsize=(18, 6))
for i in range(6):
  plt.subplot(1, 6, i+1)
  plt.axis('off')
  plt.imshow(output[i].cpu().detach().numpy())

plt.show();

"""## 2.2. Latent Representation

Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.
Наша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве.

Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве.

Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет


Итак, план:
1. Получить латентные представления картинок тестового датасета
2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)
3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр.
"""

#<ваш код получения латентных представлений, применения TSNE и визуализации>
def get_latent_representations(vae_model, loader, device):
    vae_model.eval()
    latent_representations = []
    labels_arr = []
    i = 0

    with torch.no_grad():
        for data in tqdm(loader, desc='batch',leave=False, position = 0):
            inputs, labels = data
            inputs = inputs.view(batch.size(0), -1)
            mu, logvar = vae_model.encode(inputs.to(device))
            z = vae_model.gaussian_sampler(mu, logvar)
            latent_representations.append(z)
            labels_arr.append(labels)
            i+=1

    labels_list = [int(item.detach().cpu().numpy()) for sublist in labels_arr for item in sublist]
    return torch.cat(latent_representations,dim=0).detach().cpu().numpy(),labels_list

autoencoder = autoencoder.to(device)

latent_representation, labels_list = get_latent_representations(autoencoder,test_loader, device)

from sklearn.manifold import TSNE # для снижения размерности с помощью t-SNE

# Создаем экземпляр объекта t-SNE
tsne = TSNE(n_components=2, random_state=42)

# Применяем t-SNE к латентным представлениям
latent_tsne = tsne.fit_transform(latent_representation)

colors = ['blue', 'cyan', 'green', 'yellow', 'orange', 'red', 'magenta', 'purple', 'brown', 'pink']
label_colors = [colors[l] for l in labels_list]
plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1],c=label_colors)
plt.show()

"""## 2.3. Conditional VAE

Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер.
Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица).
И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(

Хотелось бы добавить к нашему AE функцию "выдай мне рандомное число из вот этого вот класса", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность.

### Архитектура

На картинке ниже представлена архитектура простого Conditional VAE.

По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки.

То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе.

![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)

![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)

Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки.

P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе.
"""

dim_code = 16
in_features = 784

class CVAE(nn.Module):
    def __init__(self, dim_code, num_classes):

        super(CVAE, self).__init__()


        self.num_classes = num_classes
        self.flatten = nn.Flatten()

        # encoder
        self.encoder = nn.Sequential(
            nn.Linear(in_features , out_features=512),
            nn.ReLU(),
            nn.Linear(in_features = 512 , out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=dim_code*2)
        )

        # decoder
        self.decoder = nn.Sequential(

            nn.Linear(dim_code + num_classes, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=512),
            nn.ReLU(),
            nn.Linear(in_features=512, out_features=784)
        )

    def encode(self, x):
        x = x.to(device)
        x = self.encoder(x).view(-1, 2, dim_code)
        mu = x[:,0]
        logsigma = x[:,1]

        return mu, logsigma

    def gaussian_sampler(self, mu, logsigma):
        if self.training:
          std = torch.exp(0.5 * logsigma)
          eps = torch.randn_like(std)
          sample = mu + (eps * std)
          return sample
        else:
          return mu


    def decode(self, z, class_num):
        inputs = torch.cat([z, class_num], 1)
        reconstruction = self.decoder(inputs)

        return reconstruction

    def forward(self, x, class_num):
        mu, logsigma = self.encode(x)
        latent_code = self.gaussian_sampler(mu, logsigma)
        reconstruction = self.decode(latent_code, class_num)

        reconstruction = torch.sigmoid(reconstruction)

        return mu, logsigma, reconstruction

criterion = loss_vae

autoencoder = CVAE(dim_code, num_classes=10)

optimizer = torch.optim.AdamW(autoencoder.parameters(),lr=3e-4) #<Ваш любимый оптимизатор>

autoencoder = autoencoder.to(device)

from tqdm.notebook import tqdm
import torch.nn.functional as F

n_epochs = 20
train_losses = []
val_losses = []

for epoch in tqdm(range(n_epochs)):
    autoencoder.train()
    train_losses_per_epoch = []

    for x_batch, y_batch  in train_loader:

        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        x_batch = batch.view(batch.size(0), -1).to(device)
        y_batch = F.one_hot(y_batch, 10)

        optimizer.zero_grad()
        mu, logsigma, reconstruction = autoencoder(x_batch, y_batch)
        loss = criterion(x_batch, mu, logsigma, reconstruction)
        loss.backward()
        optimizer.step()
        train_losses_per_epoch.append(loss.item())

    mean_train_loss = np.mean(train_losses_per_epoch)
    train_losses.append(mean_train_loss)


    autoencoder.eval()
    val_losses_per_epoch = []
    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            x_batch = batch.view(batch.size(0), -1).to(device)
            y_batch = F.one_hot(y_batch, 10)

            mu, logsigma, reconstruction = autoencoder(x_batch, y_batch)
            loss = criterion(x_batch, mu, logsigma, reconstruction)
            val_losses_per_epoch.append(loss.item())

    mean_val_loss = np.mean(val_losses_per_epoch)
    val_losses.append(mean_val_loss)

plt.figure(figsize=(10, 7))
plt.plot(np.arange(len(train_losses)), train_losses, label='Train')
plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')

plt.xlabel('Epoch')
plt.title('MSE loss')
plt.legend()
plt.show()

autoencoder.eval()
with torch.no_grad():
    for x_batch, y_batch in test_loader:

      ground_truth = x_batch.squeeze().numpy()
      x_batch = batch.view(batch.size(0), -1)
      y_batch = F.one_hot(y_batch, 10)
      mu, logsigma, reconstruction = autoencoder(x_batch.to(device), y_batch.to(device))
      reconstruction = reconstruction.view(-1,28,28)
      reconstruction = reconstruction.cpu().detach().numpy()

      break

import matplotlib.pyplot as plt


plt.figure(figsize=(8, 20))
for i, (gt, res) in enumerate(zip(ground_truth[:5], reconstruction[:5])):
  plt.subplot(5, 2, 2*i+1)
  plt.imshow(gt)
  plt.title('before')

  plt.subplot(5, 2, 2*i+2)
  plt.imshow(res)
  plt.title('after')

"""### Latent Representations

Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)

Опять же, нужно покрасить точки в разные цвета в зависимости от класса.
"""

#<ваш код получения латентных представлений, применения TSNE и визуализации>
def get_latent_representations(cvae_model, loader, device):
    cvae_model.eval()
    latent_representations = []
    labels_arr = []
    i = 0

    with torch.no_grad():
        for data in tqdm(loader, desc='batch',leave=False, position = 0):
            inputs, labels = data
            inputs = inputs.view(batch.size(0), -1)
            labels_arr.append(labels)
            labels = F.one_hot(labels, 10)
            mu, logvar = cvae_model.encode(inputs.to(device))
            z = cvae_model.gaussian_sampler(mu, logvar)
            latent_representations.append(z)
            i+=1

    labels_list = [int(item.detach().cpu().numpy()) for sublist in labels_arr for item in sublist]
    return torch.cat(latent_representations,dim=0).detach().cpu().numpy(),labels_list

autoencoder = autoencoder.to(device)

latent_representation, labels_list = get_latent_representations(autoencoder,test_loader, device)

from sklearn.manifold import TSNE # для снижения размерности с помощью t-SNE

# Создаем экземпляр объекта t-SNE
tsne = TSNE(n_components=2, random_state=42)

# Применяем t-SNE к латентным представлениям
latent_tsne = tsne.fit_transform(latent_representation)

colors = ['blue', 'cyan', 'green', 'yellow', 'orange', 'red', 'magenta', 'purple', 'brown', 'pink']
label_colors = [colors[l] for l in labels_list]
plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1],c=label_colors)
plt.show()

"""Разница между двумя латентными представлениями весьма ощутима, выглядит так будт в латентном пространстве, сформированном CVAE меньше "контрастности", классы сильнее перемешаны"""